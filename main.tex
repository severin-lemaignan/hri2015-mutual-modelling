\documentclass{sig-alternate}

% UTF8 support
\usepackage[utf8x]{inputenc}


\usepackage{hyperref}
\usepackage{epsf,graphicx}
\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{subcaption}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning, calc}

\usepackage[draft,nomargin,footnote]{fixme}

\newcommand{\eg}{{\textit{e.g.~}}}
\newcommand{\etal}{{\textit{et al.~}}}
\newcommand{\ie}{{\textit{i.e.~}}}

% for mutual modelling in CSCL
\newcommand{\M}[3]{{\mathcal{M}(#1, #2, #3)}}
\newcommand{\model}[3]{{$\mathcal{M}(#1, #2, #3)$}}
\newcommand{\Model}[3]{{$\mathcal{M}^{\circ}(#1, #2, #3)$}}

\newcommand{\refmodel}[2]{{$\mathcal{M}(#1, #2)$}}

\newcommand{\groundingcriterion}{{$\mathcal{M}^{\circ}_{min}$}}
\newcommand{\inigrounding}{{$\mathcal{M}^{\circ}_{t_0}$}}



%
% --- Author Metadata here ---
\conferenceinfo{10th ACM/IEEE International Conference on Human-Robot Interaction}{2015 Portland, USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

%(DH) is it too soon to claim learning by teaching benefits in the title...? 
%(DH) the paper is not about "learning handwriting", it's about the robot.
\title{\LARGE \bf
Mutual Modelling in robotics: \\Inspirations for the next steps
}

% \numberofauthors{3} 
% \author{
% \alignauthor
% Séverin Lemaignan\\
% Pierre Dillenbourg\\
%    \affaddr{Computer-Human Interaction in Learning and Instruction Laboratory (CHILI)}\\
%    \affaddr{École Polytechnique Fédérale\\ de Lausanne (EPFL)}\\
%    \affaddr{CH-1015 Lausanne, Switzerland}\\
%    \email{firstname.lastname@epfl.ch}
% \alignauthor
% Claire Braboszcz\\
%    \affaddr{Laboratory for Neurology \& \\Imaging of Cognition (LabNIC)}\\
%    \affaddr{University of Geneva}\\
%    \affaddr{CH-1211 Geneva, Switzerland}\\
%    \email{claire.braboszcz@unige.ch}
% }

%(DH) my EPFL email address won't work for much longer....


\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Mutual modelling, the reciprocal ability to establish a mental model of the
other, plays a fundamental role in human interactions. This complex cognitive
skill is however difficult to fully apprehend as it encompasses multiple
neuronal, psychological and social mechanisms that are generally not easily
turned into computational models suitable for robots.

This article attempts to present in a practical way several perspectives on
mutual modelling from a range of disciplines, and reflects on how these
perspectives can be beneficial to the advancement of social cognition in
robotics. We gather here both basic tools (formalisms, models) and exemplary
experimental settings that are of relevance to robotics.

This contribution is expected to expand the corpus of knowledge readily
available to human-robot interaction research, and we believe that it may be
leveraged to build robots with better social capabilities.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Human social dynamics rely upon the ability to effectively attribute beliefs,
goals and percepts to other people. This set of meta-representational abilities
shapes what is called a theory of mind or the ability to mentalize, and leads to
mutual modelling: the reciprocal ability to establish a mental model of the
other. This lays at the core of human interactions: normal human social
interactions depend upon the recognition of other sensory perspectives, the
understanding of other mental states, and the recognition of complex non-verbal
cues of attention and emotional state.

As such, adapting and transferring these cognitive skills to social robots is an important
research objective.

Until now, however, the human-robot interaction (HRI) community has only
scratched the surface: in~\cite{scassellati2002theory}, Brian Scassellati gave
an initial account of Leslie's and Baron-Cohen's respective models of the
emergence of a theory of mind (we discuss them below) from the perspective of
robotics, but reported implementation work was limited to simple perceptual
precursors (like face detection or color saliencies detection). Since then,
research in this field has been focused on applications relying on Flavell's
\emph{Level 1}~\cite{flavell1977development} perspective-taking, \ie
perspective-taking that only requires perceptual abilities ("\emph{I see (you do
not see the book})"), and actually mostly limited to visual perception (relevant
work include Breazeal~\cite{Breazeal2006}, Trafton~\cite{Trafton2005} and
Ros~\cite{Ros2010}).

\begin{figure}[h!t]
        \centering
        \includegraphics[width=0.7\linewidth]{sally_ann}
        \caption{The false belief experiment: two puppets, ``Ann'' and
            ``Sally'' face each other, with two boxes between them. A child (the
            subject) observes. Ann puts a ball in the beige box and then leaves.
            While she is absent, Sally moves the ball to the blue box. Ann
            returns. The experimenter asks the child: \emph{Where Ann would look
            for the ball?} Without a theory of mind, the child is not able
            to ascribe false beliefs to Ann, and therefore incorrectly answers
            \emph{In the blue box}. Visual perspective taking only is sufficient
            to pass this task.}

        \label{false-beliefs}
\end{figure}

Based on perspective taking \emph{Level 1} alone, Breazeal et
al.~\cite{breazeal2009embodied} and Warnier et al.~\cite{warnier2012when}
successfully tackled the classical hallmark of theory of mind, the false belief
experiment (Figure~\ref{false-beliefs}, introduced by~\cite{wimmer1983beliefs},
original experimental setting by~\cite{Baron-Cohen1985}). They demonstrated
complete human-robot interaction scenarios where robots recognize and handle
false belief situations in dyadic or triadic interactions, and exhibit helping
behaviours that account for the missing/false beliefs of the human partners.

Those are significant achievements, also reassuring as to endow our robots with
advanced soco-cognitive capabilities.

What we would
deeper, more
intricate, levels of theory of mind may prove in reach. What about Flavell's
\emph{Level 2} perspective-taking

\section{Mutual Modelling and Developmental Psychology}

\begin{figure}
        \centering
        \includegraphics[width=0.9\columnwidth]{representation-perspective-taking}
        \caption{\emph{Visual} perspectives allow for a first level of mutual
            modelling. However, to correctly comprehend the scene,
            \emph{representation-level} perspective taking is required: what
            does the power socket means to the baby, what does the situation
            means to the mother.}

        \label{representation-level}
\end{figure}

While an important precursor, this does not address how the human
\emph{represents} its environment (Flavell's Level 2) and hence understands it:
Figure~\ref{representation-level} illustrates this difference between
perspective-taking Levels 1 and 2 in an imaginary human-robot interaction
scenario. The \emph{visual} perspective of the baby and the mother are
represented: a robot endowed with perspective-taking level 1 is able to compute
that the baby looks at the plug and the mother looks at the baby.
\emph{Representation}-level perspective taking, on the other hand, would require
the robot to represent what the socket means to the baby (an attractive
affordance), and what the baby's behaviour represents to the mother (a potential
danger).


Incidentally, the false belief experiment was proposed by Baron-Cohen in the
frame of his research on autism (he shows that autistic children seem to
actually lack a theory of mind and suggests this as the primary cause of their
social impairments), and Frith and Happé further note in ~\cite{frith1994autism}
that this specific deficit of autism has led to a large amount of research which
proved, in turn, highly beneficial to the study of the development of theory of
mind in general.

Drawing from experimental research on autism, the next logical step is to look
at other complex mind representations capabilities, what are the tasks
exhibiting them, how do they socially impair subjects who lack them, and how
would they transfer to robots.

Frith and Happé reference in~\cite{frith1994autism} 13 such tasks, identified
during the study of social cognition by autistic children. Each of them is
proposed in two versions: one does not require mentalizing, while the other does
require it. One of these tasks, for example, required children to distinguish
emotions, namely happy/sad faces on one hand (situation-based emotion), and
surprised faces on the other (belief-based emotion)~\cite{baron1993children}.
Another task, based on the \emph{penny-hiding game}, contrasts the two
conditions in terms of \emph{object occlusion} vs \emph{information
occlusion}~\cite{baron1992out}.

These tasks prototypically illustrate social meta-cognition: one need to
represent and reflect on someone else representations (and not only
perceptions), and they are not addressed by today's research on social robots.
The question that follows is therefore: notwithstanding low-level perceptual or
motor limitations, \textbf{what are the missing meta-cognitive skills required
for a robot to successfully pass these socio-cognitive tasks?}

The extensive literature on the emergence of mind modelling by infants suggests
some perspectives. Flavell relates perspective-taking Level 1 to establishing
\emph{cognitive connections} (I see, I hear, I want, I like, I fear...), in
contrast to perspective-taking Level 2 that relates to manipulating
\emph{representations }~\cite{flavell1990developmental}.  This is exemplified by
the \emph{appearance-reality} tasks, like the \emph{elephant mask} experiment
proposed in~\cite{flavell1990developmental}: 3-years old children are not able
to tell that an experimenter hidden behind a large elephant mask but who speaks
normally \emph{looks} like an elephant, \emph{sounds} like the experimenter, and
\emph{really is} the experimenter.  It appears that, while children before 4
years are able to explicitly manipulate cognitive connections (they know for
instance that these are largely independent of each other and that they can
evolve over time) and know as well that their own connections are independent of
those of other people, they do not think that one concept can \emph{seriously}
(ie, non playfully) hold several, possibly conflicting, representations.

This \emph{connection-representation} account appears to be an significant
component of a general theory of mind (one need to recognize that the same
object/concept may have different, serious, representations to then accept false
beliefs for instance), and, from an artificial intelligence perspective, raises
an interesting challenge: \textbf{how to design a meta-representational system
that supports and manipulates multiple cognitive connections \emph{and}
representations?} 

a second direction for the 'meta-cognitive foundations of mind modelling' is
Leslie's paper on the role of pretense []. I still need to investigate it

another paper of high relevance is by Frye et al. []: they use a computational
metaphor to approach theory of mind, contrasting here also
rule-based tasks with tasks requiring a theory of mind. I also need to
investigate it further.

\section{Mutual Modelling, Philosophy of Mind and Logic}


The theoretical perspective we propose to explore relies on the formalisation
and modelling of \emph{representation}-level meta-cognitive abilities through
\emph{epistemic modal logic}~\cite{hendricks2008epistemic}. Modal logics looks
at the formal representation of \emph{possible worlds} , \ie the
\emph{possibility} or \emph{necessity} of certain assertions to hold  is well
suited to build mathematical representation of situations like "\emph{the robot
knows (the baby may not know what a power socket is)}". The objective here is to
represent socio-cognitive capabilities as logical objects (based on techniques
surveyed by Verbrugge~\cite{verbrugge2009logic}) that can be reasoned about,
manipulated, and eventually transposed onto robots. Extensive literature on
modal logics and their applicability to artificial agent is available (in
particular by Shapiro, Reiter and Levesque.  Herzig~\cite{herzig2014logics}
provides a recent survey); we plan to specifically base our work on the
\emph{doxastic epistemic logic} by van Ditmarsch and
Labuschagne~\cite{vanditmarsch2007beliefs} that has been shown to be suitable
for modeling \emph{theory of mind} mechanisms.

%, and we will research how such a logic model can **represent the experimental situations** described hereafter,
%and **transpose in a principled way into a knowledge representation framework** for social robots.


\section{Mutual Modelling in Psycholinguistics and Collaborative Learning}

\subsection{Role and understanding of mutual modelling}

\emph{Computer Supported Collaborative Learning} (CSCL) researches the cognitive
mechanisms and practical techniques underpinning efficient learning in social
situations. From its very beginning, CSCL research has been following
Roschelle and Teasley suggestion~\cite{roschelle1995construction} that
collaborative learning has something to do with the process of constructing and
maintaining a \emph{shared understanding} of the task at hand. Building a shared/mutual
understanding refers to the upper class of collaborative learning situations,
those in which students should build upon each other's understanding to refine
their own understanding.  What is expected to produce learning is not the mere
fact that two students build the same understanding but the cognitive effort
they have to engage to build this shared
understanding~\cite{schwartz1995emergence}.

The construction of a shared understanding has been investigated for several
years in psycholinguistics, under the  notion of ``grounding''\footnote{Note
that the meaning of \emph{grounding} -- ensuring a shared understanding of a
situation during an interaction -- that we employ in this article must be
distinguished from its meaning in the context of \emph{symbol grounding} as
defined by Harnad~\cite{harnad1990symbol}}~\cite{clark1986referring}.  However, the
relevance of grounding mechanisms for explaining learning outcomes has been
questioned in learning sciences. The monitoring and repair of mis-understanding
explains for instance referential failures in short dialogue episodes but does
hardly predict \emph{conceptual change} (\ie the acquisition, acceptation and
integration into one's mental model of a new belief\fixme{check with Pierre})
over longer sessions~\cite{dillenbourg2006sharing}. The cumulative effect of
grounding episodes can probably be better understood from a socio-cultural
perspective: \emph{``collaborative learning is associated with the increased
    cognitive-interactional effort involved in the transition from learning to
    understand each other to learning to understand the meanings of the semiotic
    tools that constitute the mediators of interpersonal
interaction''}~\cite{baker1999role}\fixme{Ask Pierre for explanations}, and
several scholars suggest that CSCL research should go deeper in the
understanding of how partners engage into shared meaning
making~\cite{stahl2007meaning} or \emph{intersubjective} meaning
making~\cite{suthers2006technology}.

Paradoxically, while Clark's theory is somewhat too linguistic from a conceptual
change viewpoint, it is criticized at the same time as being too cognitivist by
some psycholinguists, \ie as overestimating the amount of shared knowledge and
mutual representations actually necessary to conduct a dialogue. The fundamental
issue, as old as philosophy, is the degree of coupling between the different
levels of dialogue, mostly between the lexical/syntactical level and the deeper
semantic levels. In~\cite{pickering2006alignment}, Pickering and Garrod argue
that the mutual understanding starts mostly with a \emph{superficial alignment}
at the level of the linguistic representations, due to priming mechanisms, and
that this local alignment may -- in some cases -- lead to a \emph{global
alignment} of the semantic level (\emph{deep grounding}).  For these authors,
the convergence in dialogue, and even the repair of some misunderstandings, is
explained by this mimetic behavior more than by a monitoring of each other's
knowledge: ``...interlocutors do not need to monitor and develop full common
ground as a regular, constant part of routine conversation, as it would be
unnecessary and far too costly. Establishment of full common ground is, we
argue, a specialized and non-automatic process that is used primarily in times
of difficulty (when radical misalignment becomes apparent).'' This view is
actually not incompatible with Clark's \emph{grounding
criterion}~\cite{clark1989contributing}: the degree of shared understanding that
peers need to reach depends upon the task they perform. For instance, a dialogue
between two surgeons might rely on superficial alignment if they talk about
their friends but has to guarantee accurate common grounds when talking about
which intervention will be conducted in which way on which patient.

%This interesting cognitive science debate mostly occurred outside the field of
%learning science. In education, the question is to relate these mechanisms to
%learning outcomes. Is linguistic alignment sufficient to trigger conceptual
%change? Does negotiation of meaning only occurs when partners monitor and
%diagnose each other's knowledge. If the ratio between shallow alignment and deep
%grounding depends upon the task, and if deep grounding is a condition for
%learning, then the pedagogical challenge is to design tasks that require deep
%grounding. Most empirical studies on grounding and alignment are conducted with
%tasks, which despite being qualified as ``ecologically valid'' by their authors,
%are mere referencing tasks such as asking the way to the train station or
%helping the peer to choose a picture among many. In this contribution, we
%explore several richer tasks such as arguing on a sensitive issue or building a
%concept map.

Deep grounding or shared meaning making requires some cognitive load. For Clark,
what is important is not the individual effort made by the receiver of a
communicative act, but the overall \emph{least collaborative
effort}~\cite{clark1986referring}.  The cost of producing a perfect utterance
may be higher than the cost of repairing the problems that may arise through
misunderstandings. For instance, subjects are less careful about adapting their
utterances to their partner when they know they can provide feedback on his/her
understanding~\cite{schober1993spatial}. Dillenbourg et al. introduced the
notion of \emph{optimal collaborative effort}~\cite{dillenbourg1995evolution} to
stress that misunderstanding should not be viewed as something to be avoided (if
this was possible), but as an opportunity to engage into verbalization,
explanation, negotiation, and so forth.

\subsection{A notation for mutual modelling}

Dillenbourg proposes in~\cite{sangin2007partner} a model to represent mutual
modelling situations. He uses the notation \model{A}{B}{X} to denote ``$A$ knows
that $B$ knows $X$''. This notation does not mean $A$ has an explicit,
monolithic representation of $B$: it must be understood as an abstraction
referring to complex socio-cognitive processes. Besides, he refer to the
\emph{degree of accuracy} of the model as \Model{A}{B}{X}.

He parametrizes and assesses the mutual modelling effort through 3 variables:

\begin{enumerate}

    \item Tasks vary a lot with respect to how much they require mutual
        understanding.  The \emph{grounding criterion}~\cite{clark1986referring}
        \groundingcriterion refers to
        how important it is to mutually share a piece of information $X$ to
        succeed the task $T$. It can be computed as the probability to succeed $T$
        despite the fact $X$ is not grounded. $\mathcal{M}^{\circ}_{min}(A,B,X)$
        can be estimated from the correlation between \Model{A}{B}{X} and task
        performance. 

    \item Before any specific grounding action, there is usually a non-null
        probability that $X$ is mutually understood by $A$ and $B$ (\eg $X$
        is part of $A$'s and $B$'s cultures, it is manifest to co-present
        subjects or simply there is not much space for misunderstanding
        or disagreement about $X$). He note the theoretical accuracy of
        initial grounds $\mathcal{M}^{\circ}_{t_0}(A,B,X)$.

    \item The cost of grounding $X$ refers to the physical and cognitive effort
        required to perform a grounding act $\alpha$: a verbal repair (\eg
        rephrasing), a deictic gesture, a physical move to adopt one partner's
        viewpoint, etc. This cost varies according to media
        features~\cite{clark1991grounding}.

\end{enumerate}

These notations lead to simple initial representations of mutual modelling
during interactions.

In order to make it meaningful in the context of HRI, we hereafter use $H$ to
denote a human, while $R$ stands for a robot. Figure~\ref{mm_symmetry} then
represents a dyadic human-robot interaction, with $\Delta_1$ illustrating what
Dillenbourg calls the \emph{symmetry} question ({\it is the accuracy of my model
related or not to the accuracy of your model?}).

\begin{figure}[htb]
\centering

\begin{tikzpicture}[>=latex, scale=0.5]

\draw(0,0) node[anchor=north] (A) {\scriptsize H};
\draw(10,0) node[anchor=north] (B) {\scriptsize R};
\draw(5,2) node[anchor=north] {\scriptsize \Model{H}{R}{ X}};
\draw(5,-2) node[anchor=north] {\scriptsize \Model{R}{H}{ X}};
\draw[dashed, <->] (5,1) -- (5,-1.9) node[midway, sloped, above] {$\Delta_1$};
\draw[->] (A) to[bend left] (B);
\draw[->] (B) to[bend left] (A);

\end{tikzpicture}

\caption{Mutual modelling in a dyadic interaction, $\Delta_1 =
    \Delta(\mathcal{M}^{\circ} (H,R,X),
\mathcal{M}^{\circ} (R,H,X))$}

\label{mm_symmetry}
\end{figure}


With triads (two humans $H_1$ and $H_2$ and a robot $R$), we may compute the
accuracy of 6 models \Model{H_1}{H_2}{X}, \Model{H_2}{H_1}{X},
\Model{H_1}{R}{X}, \Model{R}{H_1}{X}, \Model{R}{H_2}{X} and \Model{H_2}{R}{X}
(Figure~\ref{mm_triangles}).

\begin{figure}[htb]
\centering
\subcaptionbox{}{ 
    \begin{tikzpicture}[>=latex, scale=0.5]

    \draw(0,2) node (A) {\scriptsize $H_1$};
    \draw(0,-2) node (B) {\scriptsize $H_2$};
    \draw(10,0) node (C) {\scriptsize $R$};

    \draw(5,2) node {\scriptsize \Model{H_1}{ R}{ X}};
    \draw(5,-2) node {\scriptsize \Model{H_2}{ R}{ X}};
    \draw[dashed, <->] (3,1.2) -- (3,-1.2) node[midway, sloped, above] {$\Delta_2$};
    \draw[->] (A) to (C);
    \draw[->] (B) to (C);

    \end{tikzpicture}
}
\subcaptionbox{}{ 
    \begin{tikzpicture}[>=latex, scale=0.5]

    \draw(0,2) node (A) {\scriptsize $H_1$};
    \draw(0,-2) node (B) {\scriptsize $H_2$};
    \draw(10,0) node (C) {\scriptsize $R$};

    \draw(5,2) node {\scriptsize \Model{R}{ H_1}{ X}};
    \draw(5,-2) node {\scriptsize \Model{R}{ H_2}{ X}};
    \draw[dashed, <->] (3,1.2) -- (3,-1.2) node[midway, sloped, above]
    {$\Delta_3$};
    \draw[<-] (A) to (C);
    \draw[<-] (B) to (C);

    \end{tikzpicture}
}
\caption{Mutual modelling in a triadic interaction}

\label{mm_triangles}
\end{figure}

This leads to two \emph{triangle questions} : Do $H_1$ and $H_2$ have the same
accuracy when modelling the robot $R$? ($\Delta_2 = \Delta(\M{H_1}{C}{X},
\M{H_2}{C}{X})$), and conversely, what may lead $R$ to model more accurately
$H_1$ or $H_2$? ($\Delta_3= \Delta(\M{C}{H_1}{X}, \M{C}{H_2}{X})$).

Finally, Dillenbourg also suggests a \emph{rectangle question}: how self- versus
other modelling compares ($\Delta_4$ in Figure~\ref{mm_rectangle})? This gives
an indication of meta-cognitive skills of the agents. We can also question if
the modelling skills depend upon what aspects are being modeled ($X$ or $Y$)
which would explain vertical differences ($\Delta_5$ in
Figure~\ref{mm_rectangle}).

\begin{figure}[htb]
\centering

\begin{tikzpicture}[>=latex, scale=0.5]

    \draw(0,0) node (a) {\scriptsize \Model{A}{ B}{ X}};
    \draw(10,0) node (b) {\scriptsize \Model{A}{ A}{ X}};
    \draw(10,-4) node (c) {\scriptsize \Model{A}{ A}{ Y}};
    \draw(0,-4) node (d) {\scriptsize \Model{A}{ B}{ Y}};
    \draw[<->] (a) -- (b) node[midway, below] {$\Delta_4$};
    \draw[<->] (b) -- (c);
    \draw[<->] (c) -- (d);
    \draw[<->] (d) -- (a) node[midway, sloped, above] {$\Delta_5$};

\end{tikzpicture}

\caption{Meta-cognitive skills and domain-dependent modelling}

\label{mm_rectangle}
\end{figure}

This model, designed in the context of human collaboration, 
evidences questions that are certainly relevant to human-robot interactions.

\begin{figure}
        \centering
        \setlength{\columnsep}{0.1cm}
        \begin{multicols}{2}
            \includegraphics[width=1.0\columnwidth]{triadic_false_beliefs.pdf}

            \resizebox{\columnwidth}{!}{
            \begin{tikzpicture}[
                    >=latex,
                scale=0.5]

                \draw(0,2) node (A) {\scriptsize Sally};
                \draw(0,-2) node (B) {\scriptsize Ann};
                \draw(10,0) node (C) {\scriptsize robot};

                \draw(5,2) node {\scriptsize \Model{Sally}{ R}{ X}};
                \draw(5,-2) node {\scriptsize \Model{Ann}{ R}{ X}};
                \draw[dashed, <->] (3,1.2) -- (3,-1.2) node[midway, sloped, above] {$\Delta_2$};
                \draw[->,very thick] (A) to (C);
                \draw[->,very thick] (B) to (C);


                \draw(0,-4) node (A) {\scriptsize Sally};
                \draw(0,-8) node (B) {\scriptsize Ann};
                \draw(10,-6) node (C) {\scriptsize robot};

                \draw(5,-4) node {\scriptsize \Model{R}{ Sally}{ X}};
                \draw(5,-8) node {\scriptsize \Model{R}{ Ann}{ X}};
                \draw[dashed, <->] (3,-5.2) -- (3,-7.2) node[midway, sloped, above]
                {$\Delta_3$};
                \draw[<-,very thick] (A) to (C);
                \draw[<-,very thick] (B) to (C);

            \end{tikzpicture}
            }
        \end{multicols}

    \caption{blabla}
    \label{}
\end{figure}

\subsection{Examples of experimental settings}

\section{Mutual Modelling and Neurosciences}

\begin{itemize}
    \item models?
    \item experimental settings?
\end{itemize}

\section{Perspectives for Robotics}

\begin{itemize}
    \item concept of \emph{superficial alignment} vs \emph{global alignment} or
        \emph{deep grounding}
    \item mimetism can be a more efficient repair strategy than full grounding
    \item concept of \emph{overall least collaborative effort}: misunderstanding should not be viewed as something to systematically
        avoid (especially true for learning situations, but still applicable to
        general interaction)
    \item concept of \emph{grounding criterion}
    \item repair strategies
\end{itemize},

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgments}

This research was supported by the Swiss National Science Foundation through the
National Centre of Competence in Research Robotics.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{thebibliography}
\bibliographystyle{abbrv}
\bibliography{mutual-modelling}

%\end{thebibliography}

\end{document}
