\documentclass{sig-alternate}

% UTF8 support
\usepackage[utf8x]{inputenc}


\usepackage{hyperref}
\usepackage{epsf,graphicx}
\graphicspath{{figures/}}
\usepackage{multicol}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning, calc}


\newcommand{\eg}{{\textit{e.g.~}}}
\newcommand{\etal}{{\textit{et al.~}}}
\newcommand{\ie}{{\textit{i.e.~}}}

% for mutual modelling in CSCL
\newcommand{\Mmodel}[3]{{\mathcal{M}(#1, #2, #3)}}
\newcommand{\model}[3]{{$\mathcal{M}(#1, #2, #3)$}}
\newcommand{\Model}[3]{{$\mathcal{M}^{\circ}(#1, #2, #3)$}}


%
% --- Author Metadata here ---
\conferenceinfo{10th ACM/IEEE International Conference on Human-Robot Interaction}{2015 Portland, USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

%(DH) is it too soon to claim learning by teaching benefits in the title...? 
%(DH) the paper is not about "learning handwriting", it's about the robot.
\title{\LARGE \bf
Mutual Modelling: What other Disciplines Teach to Robotics
}

% \numberofauthors{3} 
% \author{
% \alignauthor
% Séverin Lemaignan\\
% Pierre Dillenbourg\\
%    \affaddr{Computer-Human Interaction in Learning and Instruction Laboratory (CHILI)}\\
%    \affaddr{École Polytechnique Fédérale\\ de Lausanne (EPFL)}\\
%    \affaddr{CH-1015 Lausanne, Switzerland}\\
%    \email{firstname.lastname@epfl.ch}
% \alignauthor
% Claire Braboszcz\\
%    \affaddr{Laboratory for Neurology \& \\Imaging of Cognition (LabNIC)}\\
%    \affaddr{University of Geneva}\\
%    \affaddr{CH-1211 Geneva, Switzerland}\\
%    \email{claire.braboszcz@unige.ch}
% }

%(DH) my EPFL email address won't work for much longer....


\begin{document}



\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Mutual modelling, the reciprocal ability to establish a mental model of the
other, plays a fundamental role in human interactions. This complex cognitive
skill is however difficult to fully apprehend as it encompasses multiple
neuronal, psychological and social mechanisms that are generally not easily
turned into computational models suitable for robots.

This article attempts to present in a practical way several perspectives on
mutual modelling from a range of disciplines, and reflects on how these
perspectives can be beneficial to the advancement of social cognition in
robotics. We gather here both basic tools (formalisms, models) and exemplary
experimental settings that are of relevance to robotics.

This contribution is expected to expand the corpus of knowledge readily
available to human-robot interaction research, and we believe that it may be
leveraged to build robots with better social capabilities.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Human social dynamics rely upon the ability to effectively attribute beliefs,
goals and percepts to other people. This set of meta-representational abilities
shapes what is called a theory of mind or the ability to mentalize, and leads to
mutual modelling: the reciprocal ability to establish a mental model of the
other. This lays at the core of human interactions: normal human social
interactions depend upon the recognition of other sensory perspectives, the
understanding of other mental states, and the recognition of complex non-verbal
cues of attention and emotional state.

As such, transferring these cognitive skills to social robots is an important
research objective, also explicitly underlined as being one of the EU priorities
within the Horizon 2020 framework, which emphasizes the need to endow artificial
systems with new cognitive capabilities, beyond "repetitive problem solving".
Until now, however, the human-robot interaction (HRI) community has only
scratched the surface: in~\cite{scassellati2002theory}, Brian Scassellati gives
an account of Leslie's and Baron-Cohen's respective models of the emergence of a
theory of mind from the perspective of robotics, but reported implementation
work is limited to simple perceptual precursors. Since then, research in this
field has been focused on applications relying on Flavell's \emph{Level 1}
perspective-taking, i.e. perspective-taking that only requires perceptual
abilities ("\emph{I see (you do not see the book})"), and actually mostly
limited to visual perception (relevant work include
Breazeal~\cite{Breazeal2006}, Trafton~\cite{Trafton2005} and
Ros~\cite{Ros2010}).


\section{Mutual Modelling and Developmental Psychology}

\begin{figure}[h!t]
        \centering
        \includegraphics[width=0.7\linewidth]{sally_ann}
        \caption{The False-Beliefs experiment: two puppets, "Ann" and "Sally" face each other, with two
    boxes between them. A child (the subject) observes. Ann puts a ball in the
beige box and then leaves. While she is absent, Sally moves the ball to the blue
box. Ann returns. The experimenter asks the child: \emph{Where Ann would look for
the ball?} Without a theory of mind, the child is not able to ascribe false
beliefs to Ann, and therefore incorrectly answers \emph{In the blue box}.}

        \label{false-beliefs}
\end{figure}


Based on perspective taking Level 1 alone, Breazeal et
al.~\cite{breazeal2009embodied} and Warnier et al.~\cite{warnier2012when}
successfully tackled the classical hallmark of theory of mind, the
False-Beliefs experiment (Figure~\ref{false-beliefs}, introduced
by~\cite{wimmer1983beliefs}, experimental setting by~\cite{Baron-Cohen1985}).
They demonstrated complete human-robot interaction scenarios where robots
recognize and handle false-belief situations in dyadic or triadic interactions,
and exhibit helping behaviours that account for the missing/false beliefs of the
human partners.


\begin{figure}
        \centering
        \includegraphics[width=0.9\columnwidth]{representation-perspective-taking}
        \caption{\emph{Visual} perspectives allow for a first level of mutual
            modelling. However, to correctly comprehend the scene,
            \emph{representation-level} perspective taking is required: what
            does the power socket means to the baby, what does the situation
            means to the mother.}

        \label{representation-level}
\end{figure}

While an important precursor, this does not address how the human
\emph{represents} its environment (Flavell's Level 2) and hence understands it:
Figure~\ref{representation-level} illustrates this difference between
perspective-taking Levels 1 and 2 in an imaginary human-robot interaction
scenario. The \emph{visual} perspective of the baby and the mother are
represented: a robot endowed with perspective-taking level 1 is able to compute
that the baby looks at the plug and the mother looks at the baby.
\emph{Representation}-level perspective taking, on the other hand, would require
the robot to represent what the socket means to the baby (an attractive
affordance), and what the baby's behaviour represents to the mother (a potential
danger).


Incidentally, the False-beliefs experiment was proposed by Baron-Cohen in the
frame of his research on autism (he shows that autistic children seem to
actually lack a theory of mind and suggests this as the primary cause of their
social impairments), and Frith and Happé further note in ~\cite{frith1994autism}
that this specific deficit of autism has led to a large amount of research which
proved, in turn, highly beneficial to the study of the development of theory of
mind in general.

Drawing from experimental research on autism, the next logical step is to look
at other complex mind representations capabilities, what are the tasks
exhibiting them, how do they socially impair subjects who lack them, and how
would they transfer to robots.

Frith and Happé reference in~\cite{frith1994autism} 13 such tasks, identified
during the study of social cognition by autistic children. Each of them is
proposed in two versions: one does not require mentalizing, while the other does
require it. One of these tasks, for example, required children to distinguish
emotions, namely happy/sad faces on one hand (situation-based emotion), and
surprised faces on the other (belief-based emotion)~\cite{baron1993children}.
Another task, based on the \emph{penny-hiding game}, contrasts the two
conditions in terms of \emph{object occlusion} vs \emph{information
occlusion}~\cite{baron1992out}.

These tasks prototypically illustrate social meta-cognition: one need to
represent and reflect on someone else representations (and not only
perceptions), and they are not addressed by today's research on social robots.
The question that follows is therefore: notwithstanding low-level perceptual or
motor limitations, \textbf{what are the missing meta-cognitive skills required
for a robot to successfully pass these socio-cognitive tasks?}

The extensive literature on the emergence of mind modelling by infants suggests
some perspectives. Flavell relates perspective-taking Level 1 to establishing
\emph{cognitive connections} (I see, I hear, I want, I like, I fear...), in
contrast to perspective-taking Level 2 that relates to manipulating
\emph{representations }~\cite{flavell1990developmental}.  This is exemplified by
the \emph{appearance-reality} tasks, like the \emph{elephant mask} experiment
proposed in~\cite{flavell1990developmental}: 3-years old children are not able
to tell that an experimenter hidden behind a large elephant mask but who speaks
normally \emph{looks} like an elephant, \emph{sounds} like the experimenter, and
\emph{really is} the experimenter.  It appears that, while children before 4
years are able to explicitly manipulate cognitive connections (they know for
instance that these are largely independent of each other and that they can
evolve over time) and know as well that their own connections are independent of
those of other people, they do not think that one concept can \emph{seriously}
(ie, non playfully) hold several, possibly conflicting, representations.

This \emph{connection-representation} account appears to be an significant
component of a general theory of mind (one need to recognize that the same
object/concept may have different, serious, representations to then accept false
beliefs for instance), and, from an artificial intelligence perspective, raises
an interesting challenge: \textbf{how to design a meta-representational system
that supports and manipulates multiple cognitive connections \emph{and}
representations?} 

a second direction for the 'meta-cognitive foundations of mind modelling' is
Leslie's paper on the role of pretense []. I still need to investigate it

another paper of high relevance is by Frye et al. []: they use a computational
metaphor to approach theory of mind, contrasting here also
rule-based tasks with tasks requiring a theory of mind. I also need to
investigate it further.

\section{Mutual Modelling, Philosophy of Mind and Logic}


The theoretical perspective we propose to explore relies on the formalisation
and modelling of \emph{representation}-level meta-cognitive abilities through
\emph{epistemic modal logic}~\cite{hendricks2008epistemic}. Modal logics looks
at the formal representation of \emph{possible worlds} , \ie the
\emph{possibility} or \emph{necessity} of certain assertions to hold  is well
suited to build mathematical representation of situations like "\emph{the robot
knows (the baby may not know what a power socket is)}". The objective here is to
represent socio-cognitive capabilities as logical objects (based on techniques
surveyed by Verbrugge~\cite{verbrugge2009logic}) that can be reasoned about,
manipulated, and eventually transposed onto robots. Extensive literature on
modal logics and their applicability to artificial agent is available (in
particular by Shapiro, Reiter and Levesque.  Herzig~\cite{herzig2014logics}
provides a recent survey); we plan to specifically base our work on the
\emph{doxastic epistemic logic} by van Ditmarsch and
Labuschagne~\cite{vanditmarsch2007beliefs} that has been shown to be suitable
for modeling \emph{theory of mind} mechanisms.

%, and we will research how such a logic model can **represent the experimental situations** described hereafter,
%and **transpose in a principled way into a knowledge representation framework** for social robots.


\section{Mutual Modelling in Psycholinguistics and Collaborative Learning}

\emph{Computer Supported Collaborative Learning} (CSCL) researches the cognitive
mechanisms and practical techniques underpinning efficient learning in social
situations, and from its very beginning, CSCL research has been following
Roschelle and Teasley suggestion~\cite{roschelle1995construction} that
collaborative learning has something to do with the process of constructing and
maintaining a shared understanding of the task at hand. Building a shared/mutual
understanding refers to the upper class of collaborative learning situations,
those in which students should build upon each other's understanding to refine
their own understanding.  What is expected to produce learning is not the mere
fact that two students build the same understanding but the cognitive effort
they have to engage to build this shared
understanding~\cite{schwartz1995emergence}. This effort can be observed by the
frequency of rich interactions, \ie interactions whose occurrence has been
related to learning: (self-) explanations in cognitive science (Chi ; Webb),
conflict resolution in socio-cognitive theories (Doise), mutual regulation
(Blaye,) in a Vygostkian perspective, etc. The construction of a shared
understanding has been investigated for several years in psycholinguistics,
under the  notion of ``grounding''~\cite{clark1986referring}.  However, the
relevance of grounding mechanisms for explaining learning outcomes has been
questioned. The monitoring and repair of mis-understanding explains for instance
referential failures in short dialogue episodes but does hardly predict
conceptual change over longer sessions~\cite{dillenbourg2006sharing}. The
cumulative effect of grounding episodes can probably be better understood from a
socio-cultural perspective: ``collaborative learning is associated with the
increased cognitive-interactional effort involved in the transition from
learning to understand each other to learning to understand the meanings of the
semiotic tools that constitute the mediators of interpersonal
interaction''~\cite{baker1999role}.  Several scholars suggest that CSCL research
should go deeper in the understanding of how partners engage into shared meaning
making~\cite{stahl2007meaning} or \emph{intersubjective} meaning
making~\cite{suthers2006technology}.

Paradoxically, while Clark's theory is somewhat too linguistic from a conceptual
change viewpoint, it is criticized at the same time as being too cognitivist by
some psycholinguists, \ie as overestimating the amount shared knowledge and
mutual representations actually necessary to conduct a dialogue. The fundamental
issue, as old as philosophy, is the degree of coupling between the different
levels of dialogue, mostly between the lexical / syntactical level and the
deeper semantic levels. In~\cite{pickering2006alignment}, Pickering and Garrod
argue that the mutual understanding starts mostly with a \emph{superficial
alignment} at the level of the linguistic representations, due to priming
mechanisms, and that this local alignment may -- in some cases -- lead to a
\emph{global alignment} of the semantic level (\emph{deep grounding}).  For
these authors, the convergence in dialogue, and even the repair of some
misunderstandings, is explained by this mimetic behavior more than by a
monitoring of each other's knowledge: ``...interlocutors do not need to monitor
and develop full common ground as a regular, constant part of routine
conversation, as it would be unnecessary and far too costly. Establishment of
full common ground is, we argue, a specialized and non-automatic process that is
used primarily in times of difficulty (when radical misalignment becomes
apparent).'' This view is actually not incompatible with Clark's grounding
criterion: the degree of shared understanding that peers need to reach depends
upon the task they perform. For instance, a dialogue between two surgeons might
rely on superficial alignment if they talk about their friends but has to
guarantee accurate common grounds when talking about which intervention will be
conducted in which way on which patient.  Some CSCL scholars stressed the
``illusion of shared understanding'', \ie the potential decoupling between
linguistic alignment and actual shared meanings.

This interesting cognitive science debate mostly occurred outside the field of
learning. In education, the question is to relate these mechanisms to learning
outcomes. Is linguistic alignment sufficient to trigger conceptual change? Does
negotiation of meaning only occurs when partners monitor and diagnose each
other's knowledge. If the ratio between shallow alignment and deep grounding
depends upon the task, and if deep grounding is a condition for learning, then
the pedagogical challenge is to design tasks that require deep grounding. Most
empirical studies on grounding and alignment are conducted with tasks, which
despite being qualified as ``ecologically valid'' by their authors, are mere
referencing tasks such as asking the way to the train station or helping the
peer to choose a picture among many. In this contribution, we explore several
richer tasks such as arguing on a sensitive issue or building a concept map.  

Deep grounding or shared meaning making requires some cognitive load. For
\cite{clark1986referring}, what is important is not the individual effort made
by the receiver of a communicative act, but the overall least collaborative
effort.  The cost of producing a perfect utterance may be higher than the cost
of repairing the problems that may arise through misunderstandings. For
instance, subjects are less careful about adapting their utterances to their
partner when they know they can provide feedback on his/her
understanding~\cite{schober1993spatial}. We introduced the notion of
\emph{optimal collaborative effort}~\cite{dillenbourg1995evolution} to stress
that misunderstanding should not be viewed as something to be avoided (if this
was possible), but as an opportunity to engage into verbalization, explanation,
negotiation, and so forth.

\begin{figure}
        \centering
        \setlength{\columnsep}{0.1cm}
        \begin{multicols}{2}
            \includegraphics[width=1.0\columnwidth]{triadic_false_beliefs.pdf}

            \resizebox{\columnwidth}{!}{
            \begin{tikzpicture}[
                    >=latex,
                scale=0.5]

                \draw(0,2) node (A) {\scriptsize Sally};
                \draw(0,-2) node (B) {\scriptsize Ann};
                \draw(10,0) node (C) {\scriptsize robot};

                \draw(5,2) node {\scriptsize \Model{Sally}{ R}{ X}};
                \draw(5,-2) node {\scriptsize \Model{Ann}{ R}{ X}};
                \draw[dashed, <->] (3,1.2) -- (3,-1.2) node[midway, sloped, above] {$\Delta_2$};
                \draw[->,very thick] (A) to (C);
                \draw[->,very thick] (B) to (C);


                \draw(0,-4) node (A) {\scriptsize Sally};
                \draw(0,-8) node (B) {\scriptsize Ann};
                \draw(10,-6) node (C) {\scriptsize robot};

                \draw(5,-4) node {\scriptsize \Model{R}{ Sally}{ X}};
                \draw(5,-8) node {\scriptsize \Model{R}{ Ann}{ X}};
                \draw[dashed, <->] (3,-5.2) -- (3,-7.2) node[midway, sloped, above]
                {$\Delta_3$};
                \draw[<-,very thick] (A) to (C);
                \draw[<-,very thick] (B) to (C);

            \end{tikzpicture}
            }
        \end{multicols}

    \caption{blabla}
    \label{}
\end{figure}


\section{Mutual Modelling and Neurosciences}

\begin{itemize}
    \item models?
    \item experimental settings?
\end{itemize}

\section{Perspectives for Robotics}

\begin{itemize}
    \item concept of \emph{superficial alignment} vs \emph{global alignment} or
        \emph{deep grounding}
    \item mimetism can be a more efficient repair strategy than full grounding
    \item concept of \emph{overall least collaborative effort}: misunderstanding should not be viewed as something to systematically
        avoid (especially true for learning situations, but still applicable to
        general interaction)
\end{itemize},

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgments}

This research was supported by the Swiss National Science Foundation through the
National Centre of Competence in Research Robotics.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{thebibliography}
\bibliographystyle{abbrv}
\bibliography{mutual-modelling}

%\end{thebibliography}

\end{document}
